"""
æ–°ä¹¦é€Ÿé€’æœåŠ¡

æä¾›çˆ¬è™«ç®¡ç†ã€æ•°æ®åŒæ­¥ã€ç¿»è¯‘ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼š
- ç®¡ç†å¤šä¸ªå‡ºç‰ˆç¤¾çˆ¬è™«
- åŒæ­¥æ–°ä¹¦æ•°æ®åˆ°æ•°æ®åº“
- è‡ªåŠ¨ç¿»è¯‘ä¹¦åå’Œç®€ä»‹
- ç¼“å­˜å°é¢å›¾ç‰‡
"""
import json
import logging
from datetime import datetime, timezone
from typing import Any

from ..models.database import db
from ..models.new_book import NewBook, Publisher
from .cache_service import CacheService
from .translation_service import TranslationService

logger = logging.getLogger(__name__)


# çˆ¬è™«ç±»æ˜ å°„
CRAWLER_MAP: dict[str, type] = {}


def _init_crawler_map():
    """åˆå§‹åŒ–çˆ¬è™«æ˜ å°„ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªçŽ¯ä¾èµ–ï¼‰"""
    global CRAWLER_MAP
    if CRAWLER_MAP:
        return

    # APIæ•°æ®æºçˆ¬è™«
    try:
        from .publisher_crawler.open_library import OpenLibraryCrawler
        CRAWLER_MAP['OpenLibraryCrawler'] = OpenLibraryCrawler
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ OpenLibraryCrawler")

    try:
        from .publisher_crawler.google_books import GoogleBooksCrawler
        CRAWLER_MAP['GoogleBooksCrawler'] = GoogleBooksCrawler
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ GoogleBooksCrawler")

    # å‡ºç‰ˆç¤¾çˆ¬è™«
    try:
        from .publisher_crawler.penguin_random_house import PenguinRandomHouseCrawler
        CRAWLER_MAP['PenguinRandomHouseCrawler'] = PenguinRandomHouseCrawler
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ PenguinRandomHouseCrawler")

    try:
        from .publisher_crawler.simon_schuster import SimonSchusterCrawler
        CRAWLER_MAP['SimonSchusterCrawler'] = SimonSchusterCrawler
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ SimonSchusterCrawler")

    try:
        from .publisher_crawler.hachette import HachetteCrawler
        CRAWLER_MAP['HachetteCrawler'] = HachetteCrawler
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ HachetteCrawler")

    try:
        from .publisher_crawler.harpercollins import HarperCollinsCrawler
        CRAWLER_MAP['HarperCollinsCrawler'] = HarperCollinsCrawler
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ HarperCollinsCrawler")

    try:
        from .publisher_crawler.macmillan import MacmillanCrawler
        CRAWLER_MAP['MacmillanCrawler'] = MacmillanCrawler
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ MacmillanCrawler")


class NewBookService:
    """
    æ–°ä¹¦é€Ÿé€’æœåŠ¡

    ç®¡ç†å‡ºç‰ˆç¤¾çˆ¬è™«ã€æ•°æ®åŒæ­¥å’Œç¿»è¯‘ã€‚
    """

    # é»˜è®¤å‡ºç‰ˆç¤¾é…ç½®
    DEFAULT_PUBLISHERS = [
        {
            'name': 'Google Books',
            'name_en': 'Google Books',
            'website': 'https://books.google.com',
            'crawler_class': 'GoogleBooksCrawler',
        },
        {
            'name': 'Open Library',
            'name_en': 'Open Library',
            'website': 'https://openlibrary.org',
            'crawler_class': 'OpenLibraryCrawler',
        },
        {
            'name': 'ä¼é¹…å…°ç™»',
            'name_en': 'Penguin Random House',
            'website': 'https://www.penguinrandomhouse.com',
            'crawler_class': 'PenguinRandomHouseCrawler',
        },
        {
            'name': 'è¥¿è’™èˆ’æ–¯ç‰¹',
            'name_en': 'Simon & Schuster',
            'website': 'https://www.simonandschuster.com',
            'crawler_class': 'SimonSchusterCrawler',
        },
        {
            'name': 'é˜¿æ­‡ç‰¹',
            'name_en': 'Hachette',
            'website': 'https://www.hachettebookgroup.com',
            'crawler_class': 'HachetteCrawler',
        },
        {
            'name': 'å“ˆç€æŸ¯æž—æ–¯',
            'name_en': 'HarperCollins',
            'website': 'https://www.harpercollins.com',
            'crawler_class': 'HarperCollinsCrawler',
        },
        {
            'name': 'éº¦å…‹ç±³ä¼¦',
            'name_en': 'Macmillan',
            'website': 'https://us.macmillan.com',
            'crawler_class': 'MacmillanCrawler',
        },
    ]

    def __init__(
        self,
        cache_service: CacheService | None = None,
        translation_service: TranslationService | None = None
    ):
        """
        åˆå§‹åŒ–æœåŠ¡

        Args:
            cache_service: ç¼“å­˜æœåŠ¡ï¼ˆå¯é€‰ï¼‰
            translation_service: ç¿»è¯‘æœåŠ¡ï¼ˆå¯é€‰ï¼‰
        """
        self._cache = cache_service
        self._translator = translation_service
        _init_crawler_map()

    # ==================== å‡ºç‰ˆç¤¾ç®¡ç† ====================

    def init_publishers(self) -> int:
        """
        åˆå§‹åŒ–é»˜è®¤å‡ºç‰ˆç¤¾æ•°æ®

        Returns:
            åˆ›å»ºçš„å‡ºç‰ˆç¤¾æ•°é‡
        """
        count = 0

        for pub_data in self.DEFAULT_PUBLISHERS:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = Publisher.query.filter_by(name_en=pub_data['name_en']).first()
            if existing:
                logger.info(f"ðŸ“– å‡ºç‰ˆç¤¾å·²å­˜åœ¨: {pub_data['name_en']}")
                continue

            publisher = Publisher(
                name=pub_data['name'],
                name_en=pub_data['name_en'],
                website=pub_data['website'],
                crawler_class=pub_data['crawler_class'],
                is_active=True,
            )

            db.session.add(publisher)
            count += 1
            logger.info(f"âœ… åˆ›å»ºå‡ºç‰ˆç¤¾: {pub_data['name_en']}")

        if count > 0:
            db.session.commit()
            logger.info(f"âœ… æˆåŠŸåˆ›å»º {count} ä¸ªå‡ºç‰ˆç¤¾")
        else:
            logger.info("ðŸ“– æ‰€æœ‰å‡ºç‰ˆç¤¾å·²å­˜åœ¨ï¼Œæ— éœ€åˆ›å»º")

        return count

    def get_publishers(self, active_only: bool = True) -> list[Publisher]:
        """
        èŽ·å–å‡ºç‰ˆç¤¾åˆ—è¡¨

        Args:
            active_only: æ˜¯å¦åªè¿”å›žå¯ç”¨çš„å‡ºç‰ˆç¤¾

        Returns:
            å‡ºç‰ˆç¤¾åˆ—è¡¨
        """
        query = Publisher.query

        if active_only:
            query = query.filter_by(is_active=True)

        return query.order_by(Publisher.name_en).all()

    def get_publisher(self, publisher_id: int) -> Publisher | None:
        """
        èŽ·å–å•ä¸ªå‡ºç‰ˆç¤¾

        Args:
            publisher_id: å‡ºç‰ˆç¤¾ID

        Returns:
            å‡ºç‰ˆç¤¾å¯¹è±¡æˆ–None
        """
        return Publisher.query.get(publisher_id)

    def update_publisher_status(self, publisher_id: int, is_active: bool) -> bool:
        """
        æ›´æ–°å‡ºç‰ˆç¤¾çŠ¶æ€

        Args:
            publisher_id: å‡ºç‰ˆç¤¾ID
            is_active: æ˜¯å¦å¯ç”¨

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        publisher = self.get_publisher(publisher_id)
        if not publisher:
            return False

        publisher.is_active = is_active
        db.session.commit()
        logger.info(f"âœ… æ›´æ–°å‡ºç‰ˆç¤¾çŠ¶æ€: {publisher.name_en} -> {'å¯ç”¨' if is_active else 'ç¦ç”¨'}")
        return True

    # ==================== çˆ¬è™«ç®¡ç† ====================

    def get_crawler(self, crawler_class: str):
        """
        èŽ·å–çˆ¬è™«å®žä¾‹

        Args:
            crawler_class: çˆ¬è™«ç±»å

        Returns:
            çˆ¬è™«å®žä¾‹æˆ–None
        """
        crawler_cls = CRAWLER_MAP.get(crawler_class)
        if not crawler_cls:
            logger.error(f"âŒ æœªæ‰¾åˆ°çˆ¬è™«ç±»: {crawler_class}")
            return None

        # Google Books çˆ¬è™«éœ€è¦ API key
        if crawler_class == 'GoogleBooksCrawler':
            from flask import current_app
            api_key = current_app.config.get('GOOGLE_API_KEY') if current_app else None
            if api_key:
                from .publisher_crawler.base_crawler import CrawlerConfig
                config = CrawlerConfig(api_key=api_key)
                return crawler_cls(config)

        return crawler_cls()

    def sync_publisher_books(
        self,
        publisher_id: int,
        category: str | None = None,
        max_books: int = 50,
        translate: bool = True
    ) -> dict[str, Any]:
        """
        åŒæ­¥æŒ‡å®šå‡ºç‰ˆç¤¾çš„æ–°ä¹¦æ•°æ®

        Args:
            publisher_id: å‡ºç‰ˆç¤¾ID
            category: åˆ†ç±»ç­›é€‰
            max_books: æœ€å¤§åŒæ­¥æ•°é‡
            translate: æ˜¯å¦ç¿»è¯‘

        Returns:
            åŒæ­¥ç»“æžœç»Ÿè®¡
        """
        publisher = self.get_publisher(publisher_id)
        if not publisher:
            return {'success': False, 'error': 'å‡ºç‰ˆç¤¾ä¸å­˜åœ¨'}

        if not publisher.is_active:
            return {'success': False, 'error': 'å‡ºç‰ˆç¤¾å·²ç¦ç”¨'}

        crawler = self.get_crawler(publisher.crawler_class)
        if not crawler:
            return {'success': False, 'error': 'çˆ¬è™«ä¸å¯ç”¨'}

        result = {
            'success': True,
            'publisher': publisher.name_en,
            'total': 0,
            'added': 0,
            'updated': 0,
            'skipped': 0,
            'errors': 0,
        }

        try:
            logger.info(f"ðŸ” å¼€å§‹åŒæ­¥ {publisher.name_en} æ–°ä¹¦...")

            with crawler:
                for book_info in crawler.get_new_books(category=category, max_books=max_books):
                    result['total'] += 1

                    try:
                        # ä¿å­˜æˆ–æ›´æ–°ä¹¦ç±
                        save_result = self._save_book(publisher, book_info, translate)

                        if save_result == 'added':
                            result['added'] += 1
                        elif save_result == 'updated':
                            result['updated'] += 1
                        else:
                            result['skipped'] += 1

                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜ä¹¦ç±å¤±è´¥: {book_info.title} - {e}")
                        result['errors'] += 1

            # æ›´æ–°åŒæ­¥æ—¶é—´
            publisher.last_sync_at = datetime.now(timezone.utc)
            publisher.sync_count += 1
            db.session.commit()

            logger.info(
                f"âœ… åŒæ­¥å®Œæˆ: {publisher.name_en} - "
                f"æ€»è®¡ {result['total']}, æ–°å¢ž {result['added']}, "
                f"æ›´æ–° {result['updated']}, è·³è¿‡ {result['skipped']}"
            )

        except Exception as e:
            logger.error(f"âŒ åŒæ­¥å¤±è´¥: {e}")
            result['success'] = False
            result['error'] = str(e)

        return result

    def sync_all_publishers(
        self,
        category: str | None = None,
        max_books_per_publisher: int = 30,
        translate: bool = True
    ) -> list[dict[str, Any]]:
        """
        åŒæ­¥æ‰€æœ‰å¯ç”¨å‡ºç‰ˆç¤¾çš„æ–°ä¹¦æ•°æ®

        Args:
            category: åˆ†ç±»ç­›é€‰
            max_books_per_publisher: æ¯ä¸ªå‡ºç‰ˆç¤¾æœ€å¤§åŒæ­¥æ•°é‡
            translate: æ˜¯å¦ç¿»è¯‘

        Returns:
            å„å‡ºç‰ˆç¤¾åŒæ­¥ç»“æžœåˆ—è¡¨
        """
        results = []
        publishers = self.get_publishers(active_only=True)

        logger.info(f"ðŸ” å¼€å§‹åŒæ­¥ {len(publishers)} ä¸ªå‡ºç‰ˆç¤¾...")

        for publisher in publishers:
            result = self.sync_publisher_books(
                publisher.id,
                category=category,
                max_books=max_books_per_publisher,
                translate=translate
            )
            results.append(result)

        # æ±‡æ€»ç»Ÿè®¡
        total_added = sum(r.get('added', 0) for r in results)
        total_updated = sum(r.get('updated', 0) for r in results)
        total_errors = sum(r.get('errors', 0) for r in results)

        logger.info(
            f"âœ… å…¨éƒ¨åŒæ­¥å®Œæˆ: æ–°å¢ž {total_added}, æ›´æ–° {total_updated}, é”™è¯¯ {total_errors}"
        )

        return results

    # ==================== ä¹¦ç±ç®¡ç† ====================

    def _save_book(
        self,
        publisher: Publisher,
        book_info,
        translate: bool = True
    ) -> str:
        """
        ä¿å­˜ä¹¦ç±åˆ°æ•°æ®åº“

        Args:
            publisher: å‡ºç‰ˆç¤¾å¯¹è±¡
            book_info: ä¹¦ç±ä¿¡æ¯
            translate: æ˜¯å¦ç¿»è¯‘

        Returns:
            æ“ä½œç»“æžœ: 'added', 'updated', 'skipped'
        """
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé€šè¿‡ISBNæˆ–æ ‡é¢˜+ä½œè€…ï¼‰
        existing = None

        if book_info.isbn13:
            existing = NewBook.query.filter_by(
                publisher_id=publisher.id,
                isbn13=book_info.isbn13
            ).first()

        if not existing and book_info.isbn10:
            existing = NewBook.query.filter_by(
                publisher_id=publisher.id,
                isbn10=book_info.isbn10
            ).first()

        if not existing:
            # é€šè¿‡æ ‡é¢˜å’Œä½œè€…åŒ¹é…
            existing = NewBook.query.filter_by(
                publisher_id=publisher.id,
                title=book_info.title,
                author=book_info.author
            ).first()

        if existing:
            # æ›´æ–°çŽ°æœ‰è®°å½•
            updated = self._update_book_fields(existing, book_info)
            if updated:
                return 'updated'
            return 'skipped'

        # åˆ›å»ºæ–°è®°å½•
        new_book = NewBook(
            publisher_id=publisher.id,
            title=book_info.title,
            author=book_info.author,
            isbn13=book_info.isbn13,
            isbn10=book_info.isbn10,
            description=book_info.description,
            cover_url=book_info.cover_url,
            category=book_info.category,
            publication_date=book_info.publication_date,
            price=book_info.price,
            page_count=book_info.page_count,
            language=book_info.language,
            source_url=book_info.source_url,
        )

        # è®¾ç½®è´­ä¹°é“¾æŽ¥
        if book_info.buy_links:
            new_book.set_buy_links(book_info.buy_links)

        # ç¿»è¯‘
        if translate and self._translator:
            self._translate_book(new_book)

        db.session.add(new_book)
        db.session.commit()

        return 'added'

    def _update_book_fields(self, book: NewBook, book_info) -> bool:
        """
        æ›´æ–°ä¹¦ç±å­—æ®µ

        Args:
            book: ä¹¦ç±å¯¹è±¡
            book_info: æ–°çš„ä¹¦ç±ä¿¡æ¯

        Returns:
            æ˜¯å¦æœ‰æ›´æ–°
        """
        updated = False

        # æ›´æ–°å¯èƒ½å˜åŒ–çš„å­—æ®µ
        if book_info.description and book_info.description != book.description:
            book.description = book_info.description
            updated = True

        if book_info.cover_url and book_info.cover_url != book.cover_url:
            book.cover_url = book_info.cover_url
            updated = True

        if book_info.price and book_info.price != book.price:
            book.price = book_info.price
            updated = True

        if book_info.buy_links:
            book.set_buy_links(book_info.buy_links)
            updated = True

        if updated:
            book.updated_at = datetime.now(timezone.utc)
            db.session.commit()

        return updated

    def _translate_book(self, book: NewBook) -> None:
        """
        ç¿»è¯‘ä¹¦ç±ä¿¡æ¯

        Args:
            book: ä¹¦ç±å¯¹è±¡
        """
        if not self._translator:
            return

        try:
            # ç¿»è¯‘ä¹¦å
            if book.title and not book.title_zh:
                book.title_zh = self._translator.translate(book.title, 'en', 'zh')

            # ç¿»è¯‘ç®€ä»‹
            if book.description and not book.description_zh:
                book.description_zh = self._translator.translate(
                    book.description[:1000],  # é™åˆ¶é•¿åº¦
                    'en',
                    'zh'
                )

        except Exception as e:
            logger.warning(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {book.title} - {e}")

    # ==================== æŸ¥è¯¢æŽ¥å£ ====================

    def get_new_books(
        self,
        publisher_id: int | None = None,
        category: str | None = None,
        days: int = 30,
        page: int = 1,
        per_page: int = 20
    ) -> tuple[list[NewBook], int]:
        """
        èŽ·å–æ–°ä¹¦åˆ—è¡¨

        Args:
            publisher_id: å‡ºç‰ˆç¤¾IDç­›é€‰
            category: åˆ†ç±»ç­›é€‰
            days: æœ€è¿‘å¤©æ•°
            page: é¡µç 
            per_page: æ¯é¡µæ•°é‡

        Returns:
            (ä¹¦ç±åˆ—è¡¨, æ€»æ•°)
        """
        from datetime import timedelta

        query = NewBook.query.filter(NewBook.is_displayable == True)

        # æ—¶é—´ç­›é€‰
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
        query = query.filter(NewBook.created_at >= cutoff_date)

        # å‡ºç‰ˆç¤¾ç­›é€‰
        if publisher_id:
            query = query.filter(NewBook.publisher_id == publisher_id)

        # åˆ†ç±»ç­›é€‰
        if category:
            query = query.filter(NewBook.category == category)

        # æŽ’åº
        query = query.order_by(NewBook.publication_date.desc().nullslast(), NewBook.created_at.desc())

        # åˆ†é¡µ
        pagination = query.paginate(page=page, per_page=per_page, error_out=False)

        return pagination.items, pagination.total

    def get_book(self, book_id: int) -> NewBook | None:
        """
        èŽ·å–å•æœ¬ä¹¦ç±è¯¦æƒ…

        Args:
            book_id: ä¹¦ç±ID

        Returns:
            ä¹¦ç±å¯¹è±¡æˆ–None
        """
        return NewBook.query.get(book_id)

    def search_books(
        self,
        keyword: str,
        page: int = 1,
        per_page: int = 20
    ) -> tuple[list[NewBook], int]:
        """
        æœç´¢ä¹¦ç±

        Args:
            keyword: æœç´¢å…³é”®è¯
            page: é¡µç 
            per_page: æ¯é¡µæ•°é‡

        Returns:
            (ä¹¦ç±åˆ—è¡¨, æ€»æ•°)
        """
        search_pattern = f"%{keyword}%"

        query = NewBook.query.filter(
            db.or_(
                NewBook.title.ilike(search_pattern),
                NewBook.title_zh.ilike(search_pattern),
                NewBook.author.ilike(search_pattern),
                NewBook.isbn13 == keyword,
                NewBook.isbn10 == keyword,
            )
        ).filter(NewBook.is_displayable == True)

        query = query.order_by(NewBook.created_at.desc())

        pagination = query.paginate(page=page, per_page=per_page, error_out=False)

        return pagination.items, pagination.total

    def get_categories(self) -> list[dict[str, str]]:
        """
        èŽ·å–æ‰€æœ‰åˆ†ç±»

        Returns:
            åˆ†ç±»åˆ—è¡¨
        """
        from sqlalchemy import func

        results = db.session.query(
            NewBook.category,
            func.count(NewBook.id).label('count')
        ).filter(
            NewBook.category.isnot(None),
            NewBook.is_displayable == True
        ).group_by(
            NewBook.category
        ).order_by(
            func.count(NewBook.id).desc()
        ).all()

        return [
            {'name': r.category, 'count': r.count}
            for r in results
        ]

    # ==================== ç»Ÿè®¡æŽ¥å£ ====================

    def get_statistics(self) -> dict[str, Any]:
        """
        èŽ·å–ç»Ÿè®¡æ•°æ®

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        from sqlalchemy import func

        # æ€»ä½“ç»Ÿè®¡
        total_books = NewBook.query.count()
        total_publishers = Publisher.query.count()
        active_publishers = Publisher.query.filter_by(is_active=True).count()

        # æœ€è¿‘7å¤©æ–°å¢ž
        from datetime import timedelta
        week_ago = datetime.now(timezone.utc) - timedelta(days=7)
        recent_books = NewBook.query.filter(NewBook.created_at >= week_ago).count()

        # åˆ†ç±»ç»Ÿè®¡
        category_stats = db.session.query(
            NewBook.category,
            func.count(NewBook.id).label('count')
        ).filter(
            NewBook.category.isnot(None)
        ).group_by(
            NewBook.category
        ).order_by(
            func.count(NewBook.id).desc()
        ).limit(10).all()

        return {
            'total_books': total_books,
            'total_publishers': total_publishers,
            'active_publishers': active_publishers,
            'recent_books_7d': recent_books,
            'top_categories': [
                {'category': c.category, 'count': c.count}
                for c in category_stats
            ],
        }
