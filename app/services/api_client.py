import time
import logging
import hashlib
from pathlib import Path
from functools import wraps
from typing import Any

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from ..utils.exceptions import APIRateLimitException, APIException
from ..utils.rate_limiter import RateLimiter

logger = logging.getLogger(__name__)


def retry(max_attempts: int = 3, backoff_factor: float = 2.0,
          exceptions=(requests.RequestException,)):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    if attempt == max_attempts - 1:
                        raise
                    wait_time = backoff_factor ** attempt
                    logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
                    time.sleep(wait_time)
        return wrapper
    return decorator


def create_session_with_retry(max_retries: int = 3, backoff_factor: float = 0.5) -> requests.Session:
    """
    åˆ›å»ºé…ç½®äº†é‡è¯•æœºåˆ¶çš„ requests Session

    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        backoff_factor: é€€é¿å› å­

    Returns:
        é…ç½®å¥½çš„ Session å¯¹è±¡
    """
    session = requests.Session()

    # é…ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=max_retries,
        backoff_factor=backoff_factor,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"]
    )

    # é…ç½®è¿æ¥æ± 
    adapter = HTTPAdapter(
        pool_connections=10,
        pool_maxsize=20,
        max_retries=retry_strategy
    )

    session.mount("http://", adapter)
    session.mount("https://", adapter)

    # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
    session.headers.update({
        'User-Agent': 'BookRank/2.0 (https://github.com/gongyijie85/bookrank)',
        'Accept': 'application/json',
    })

    return session


class NYTApiClient:
    """çº½çº¦æ—¶æŠ¥APIå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str, base_url: str, rate_limiter: RateLimiter, timeout: int = 15):
        self._api_key = api_key
        self._base_url = base_url
        self._rate_limiter = rate_limiter
        self._timeout = timeout
        # ä½¿ç”¨é…ç½®äº†é‡è¯•æœºåˆ¶çš„ Session
        self._session = create_session_with_retry(max_retries=3)

    @retry(max_attempts=3, backoff_factor=2.0)
    def fetch_books(self, category_id: str) -> dict[str, Any]:
        """
        è·å–æŒ‡å®šåˆ†ç±»çš„å›¾ä¹¦æ•°æ®

        Args:
            category_id: åˆ†ç±»ID

        Returns:
            APIå“åº”æ•°æ®

        Raises:
            APIRateLimitException: å½“APIé™æµæ—¶
            APIException: å½“APIè°ƒç”¨å¤±è´¥æ—¶
        """
        if not self._api_key:
            raise APIException("NYT API key not configured", 500)

        # æ£€æŸ¥é™æµ
        if not self._rate_limiter.is_allowed():
            retry_after = self._rate_limiter.get_retry_after()
            raise APIRateLimitException(
                f"API rate limit exceeded. Retry after {retry_after}s",
                retry_after
            )

        url = f"{self._base_url}/{category_id}.json"

        try:
            response = self._session.get(
                url,
                params={'api-key': self._api_key},
                timeout=self._timeout
            )

            # å¤„ç†é™æµå“åº”
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                raise APIRateLimitException("API rate limited", retry_after)

            response.raise_for_status()
            return response.json()

        except requests.Timeout:
            raise APIException(f"Request timeout for {category_id}", 504)
        except requests.RequestException as e:
            raise APIException(f"API request failed: {str(e)}", 502)


class GoogleBooksClient:
    """Google Books APIå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str | None, base_url: str, timeout: int = 8):
        self._api_key = api_key
        self._base_url = base_url
        self._timeout = timeout
        # ä½¿ç”¨é…ç½®äº†é‡è¯•æœºåˆ¶çš„ Session
        self._session = create_session_with_retry(max_retries=2)

    @retry(max_attempts=2, backoff_factor=1.5)
    def fetch_book_details(self, isbn: str) -> dict[str, Any]:
        """
        è·å–å›¾ä¹¦è¯¦ç»†ä¿¡æ¯

        Args:
            isbn: å›¾ä¹¦ISBN

        Returns:
            å›¾ä¹¦è¯¦ç»†ä¿¡æ¯å­—å…¸
        """
        if not isbn:
            return {}

        url = f"{self._base_url}"
        params = {
            'q': f'isbn:{isbn}'
        }
        # API Key æ˜¯å¯é€‰çš„
        if self._api_key:
            params['key'] = self._api_key

        try:
            response = self._session.get(
                url,
                params=params,
                timeout=self._timeout
            )
            response.raise_for_status()
            data = response.json()

            if 'items' not in data or len(data['items']) == 0:
                return {}

            return self._parse_volume_info(data['items'][0]['volumeInfo'])

        except requests.RequestException as e:
            logger.warning(f"Failed to fetch Google Books data for ISBN {isbn}: {e}")
            return {}

    @retry(max_attempts=2, backoff_factor=1.5)
    def search_book_by_title(self, title: str, author: str = None) -> dict[str, Any]:
        """
        æ ¹æ®ä¹¦åæœç´¢å›¾ä¹¦

        Args:
            title: å›¾ä¹¦æ ‡é¢˜
            author: ä½œè€…ï¼ˆå¯é€‰ï¼Œç”¨äºæé«˜æœç´¢ç²¾åº¦ï¼‰

        Returns:
            å›¾ä¹¦è¯¦ç»†ä¿¡æ¯å­—å…¸
        """
        if not title:
            return {}

        url = f"{self._base_url}"
        # æ„å»ºæœç´¢æŸ¥è¯¢
        query = f'intitle:{title}'
        if author:
            query += f' inauthor:{author}'

        params = {
            'q': query,
            'maxResults': 1
        }
        if self._api_key:
            params['key'] = self._api_key

        try:
            response = self._session.get(
                url,
                params=params,
                timeout=self._timeout
            )
            response.raise_for_status()
            data = response.json()

            if 'items' not in data or len(data['items']) == 0:
                return {}

            return self._parse_volume_info(data['items'][0]['volumeInfo'])

        except requests.RequestException as e:
            logger.warning(f"Failed to search Google Books for '{title}': {e}")
            return {}
    
    def _parse_volume_info(self, volume_info: dict[str, Any]) -> dict[str, Any]:
        """è§£æ Google Books API è¿”å›çš„ volumeInfo"""
        lang_code = volume_info.get('language', '').lower()
        
        from ..config import Config
        
        # è·å–å°é¢å›¾ç‰‡URLï¼ˆä¼˜å…ˆä½¿ç”¨å¤§å°ºå¯¸çš„ï¼‰
        image_links = volume_info.get('imageLinks', {})
        cover_url = (image_links.get('extraLarge') or 
                    image_links.get('large') or 
                    image_links.get('medium') or 
                    image_links.get('small') or 
                    image_links.get('thumbnail') or 
                    image_links.get('smallThumbnail'))
        
        # å¤„ç† HTTP å›¾ç‰‡URLï¼ˆè½¬æ¢ä¸ºHTTPSï¼‰
        if cover_url and cover_url.startswith('http:'):
            cover_url = 'https:' + cover_url[5:]
        
        return {
            'title': volume_info.get('title'),
            'authors': volume_info.get('authors', []),
            'publication_dt': volume_info.get('publishedDate', 'Unknown'),
            'details': volume_info.get('description', 'No detailed description available.'),
            'page_count': volume_info.get('pageCount', 'Unknown'),
            'language': Config.LANGUAGE_MAP.get(lang_code, lang_code),
            'cover_url': cover_url,
            'isbn_13': self._extract_isbn(volume_info, 'ISBN_13'),
            'isbn_10': self._extract_isbn(volume_info, 'ISBN_10'),
            'publisher': volume_info.get('publisher')
        }
    
    def _extract_isbn(self, volume_info: dict[str, Any], isbn_type: str) -> str | None:
        """ä» volumeInfo ä¸­æå– ISBN"""
        identifiers = volume_info.get('industryIdentifiers', [])
        for identifier in identifiers:
            if identifier.get('type') == isbn_type:
                return identifier.get('identifier')
        return None
    
    def get_cover_url(self, isbn: str = None, title: str = None, author: str = None) -> str | None:
        """
        è·å–å›¾ä¹¦å°é¢URL
        
        Args:
            isbn: ISBNï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            title: ä¹¦åï¼ˆå½“ISBNæœç´¢å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
            author: ä½œè€…ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å°é¢å›¾ç‰‡URLæˆ–None
        """
        # ä¼˜å…ˆä½¿ç”¨ISBNæœç´¢
        if isbn:
            details = self.fetch_book_details(isbn)
            if details and details.get('cover_url'):
                return details['cover_url']
        
        # å¦‚æœISBNæœç´¢å¤±è´¥ï¼Œä½¿ç”¨ä¹¦åæœç´¢
        if title:
            details = self.search_book_by_title(title, author)
            if details and details.get('cover_url'):
                return details['cover_url']
        
        return None


class OpenLibraryClient:
    """
    Open Library API å®¢æˆ·ç«¯

    Open Library æ˜¯ç”± Internet Archive ç»´æŠ¤çš„å…è´¹å›¾ä¹¦æ•°æ®åº“
    ä¼˜åŠ¿ï¼šå®Œå…¨å…è´¹ï¼Œæ— éœ€ API Keyï¼Œæ”¯æŒ ISBN æŸ¥è¯¢å’Œå°é¢å›¾ç‰‡

    API æ–‡æ¡£ï¼šhttps://openlibrary.org/dev/docs/api/books
    """

    def __init__(self, timeout: int = 10):
        self._base_url = 'https://openlibrary.org'
        self._covers_url = 'https://covers.openlibrary.org'
        self._timeout = timeout
        # ä½¿ç”¨é…ç½®äº†é‡è¯•æœºåˆ¶çš„ Session
        self._session = create_session_with_retry(max_retries=2)
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œé¿å…è¢«é˜»æ­¢
        self._session.headers.update({
            'User-Agent': 'BookRank/2.0 (bookrank@example.com)'
        })
    
    @retry(max_attempts=2, backoff_factor=1.5)
    def fetch_book_by_isbn(self, isbn: str) -> dict[str, Any]:
        """
        é€šè¿‡ ISBN è·å–å›¾ä¹¦è¯¦æƒ…
        
        Args:
            isbn: å›¾ä¹¦ ISBN-10 æˆ– ISBN-13
            
        Returns:
            å›¾ä¹¦è¯¦ç»†ä¿¡æ¯å­—å…¸
        """
        if not isbn:
            return {}
        
        # æ¸…ç† ISBNï¼ˆç§»é™¤è¿å­—ç¬¦å’Œç©ºæ ¼ï¼‰
        clean_isbn = isbn.replace('-', '').replace(' ', '')
        
        url = f"{self._base_url}/api/books"
        params = {
            'bibkeys': f'ISBN:{clean_isbn}',
            'format': 'json',
            'jscmd': 'data'
        }
        
        try:
            response = self._session.get(
                url,
                params=params,
                timeout=self._timeout
            )
            response.raise_for_status()
            data = response.json()
            
            # Open Library è¿”å›çš„é”®æ ¼å¼ä¸º "ISBN:xxxx"
            key = f'ISBN:{clean_isbn}'
            if key not in data:
                logger.warning(f"No data found for ISBN: {isbn}")
                return {}
            
            book_data = data[key]
            return self._parse_book_data(book_data, clean_isbn)
            
        except requests.RequestException as e:
            logger.warning(f"Failed to fetch Open Library data for ISBN {isbn}: {e}")
            return {}
    
    def _parse_book_data(self, book_data: dict[str, Any], isbn: str) -> dict[str, Any]:
        """è§£æ Open Library è¿”å›çš„å›¾ä¹¦æ•°æ®"""
        
        authors = []
        if 'authors' in book_data:
            authors = [author.get('name', '') for author in book_data['authors']]
        
        publish_date = book_data.get('publish_date', 'Unknown')
        publishers = []
        if 'publishers' in book_data:
            publishers = [pub.get('name', '') for pub in book_data['publishers']]
        
        cover_url = None
        if 'cover' in book_data:
            cover = book_data['cover']
            cover_url = (cover.get('large') or 
                        cover.get('medium') or 
                        cover.get('small'))
        
        pages = book_data.get('number_of_pages', 'Unknown')
        
        description = ''
        if 'description' in book_data:
            desc = book_data['description']
            if isinstance(desc, dict):
                description = desc.get('value', '')
            else:
                description = str(desc)
        
        return {
            'title': book_data.get('title'),
            'authors': authors,
            'author': ', '.join(authors) if authors else None,
            'publisher': publishers[0] if publishers else None,
            'publish_date': publish_date,
            'pages': pages,
            'description': description or 'No description available.',
            'cover_url': cover_url,
            'isbn_13': isbn if len(isbn) == 13 else None,
            'isbn_10': isbn if len(isbn) == 10 else None,
            'source': 'open_library'
        }
    
    def get_cover_url(self, isbn: str, size: str = 'L') -> str | None:
        """
        è·å– Open Library å°é¢å›¾ç‰‡ URL
        
        Args:
            isbn: å›¾ä¹¦ ISBN
            size: å›¾ç‰‡å°ºå¯¸ ('S'=å°, 'M'=ä¸­, 'L'=å¤§)
            
        Returns:
            å°é¢å›¾ç‰‡ URL æˆ– None
        """
        if not isbn:
            return None
        
        # æ¸…ç† ISBN
        clean_isbn = isbn.replace('-', '').replace(' ', '')
        
        # éªŒè¯å°ºå¯¸å‚æ•°
        size = size.upper()
        if size not in ['S', 'M', 'L']:
            size = 'L'
        
        # æ„å»ºå°é¢ URL
        cover_url = f"{self._covers_url}/b/isbn/{clean_isbn}-{size}.jpg"
        
        # æ£€æŸ¥å°é¢æ˜¯å¦å­˜åœ¨ï¼ˆOpen Library ä¼šè¿”å› 1x1 åƒç´ çš„å ä½å›¾å¦‚æœä¸å­˜åœ¨ï¼‰
        try:
            response = self._session.head(cover_url, timeout=5)
            if response.status_code == 200:
                # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œæ’é™¤å ä½å›¾
                content_length = response.headers.get('Content-Length')
                if content_length and int(content_length) > 100:
                    return cover_url
        except requests.RequestException:
            pass
        
        return None
    
    def search_books(self, query: str, limit: int = 10) -> list:
        """
        æœç´¢å›¾ä¹¦
        
        Args:
            query: æœç´¢å…³é”®è¯
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            å›¾ä¹¦åˆ—è¡¨
        """
        url = f"{self._base_url}/search.json"
        params = {
            'q': query,
            'limit': limit
        }
        
        try:
            response = self._session.get(
                url,
                params=params,
                timeout=self._timeout
            )
            response.raise_for_status()
            data = response.json()
            
            books = []
            for doc in data.get('docs', []):
                book = {
                    'title': doc.get('title'),
                    'authors': doc.get('author_name', []),
                    'author': ', '.join(doc.get('author_name', [])),
                    'first_publish_year': doc.get('first_publish_year'),
                    'isbn': doc.get('isbn', [None])[0] if doc.get('isbn') else None,
                    'cover_id': doc.get('cover_i')
                }
                books.append(book)
            
            return books
            
        except requests.RequestException as e:
            logger.warning(f"Failed to search Open Library: {e}")
            return []


class WikidataClient:
    """
    Wikidata SPARQL API å®¢æˆ·ç«¯

    ç”¨äºæ‰¹é‡è·å–å›¾ä¹¦å¥–é¡¹è·å¥–æ•°æ®
    Wikidata æ˜¯ç»´åŸºç™¾ç§‘çš„ç»“æ„åŒ–æ•°æ®å­˜å‚¨åº“

    API æ–‡æ¡£ï¼šhttps://www.wikidata.org/wiki/Wikidata:SPARQL_query_service
    """

    # å¥–é¡¹çš„ Wikidata QID
    AWARD_IDS = {
        'nebula': 'Q327503',           # æ˜Ÿäº‘å¥–
        'hugo': 'Q162455',             # é›¨æœå¥–
        'booker': 'Q155091',           # å¸ƒå…‹å¥–
        'international_booker': 'Q2519161',  # å›½é™…å¸ƒå…‹å¥–
        'pulitzer_fiction': 'Q162530', # æ™®åˆ©ç­–å°è¯´å¥–
        'edgar': 'Q532244',            # çˆ±ä¼¦Â·å¡å¥–
        'nobel_literature': 'Q37922',  # è¯ºè´å°”æ–‡å­¦å¥–
    }

    def __init__(self, timeout: int = 30):
        self._base_url = 'https://query.wikidata.org/sparql'
        self._timeout = timeout
        # ä½¿ç”¨é…ç½®äº†é‡è¯•æœºåˆ¶çš„ Session
        self._session = create_session_with_retry(max_retries=2)
        self._session.headers.update({
            'User-Agent': 'BookRank/2.0 (bookrank@example.com)',
            'Accept': 'application/sparql-results+json'
        })
    
    @retry(max_attempts=2, backoff_factor=1.5)
    def query_award_winners(self, award_key: str, start_year: int = 2020, 
                           end_year: int = 2025, limit: int = 100) -> list:
        """
        æŸ¥è¯¢æŒ‡å®šå¥–é¡¹çš„è·å¥–å›¾ä¹¦
        
        Args:
            award_key: å¥–é¡¹é”®å (nebula, hugo, booker ç­‰)
            start_year: å¼€å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            è·å¥–å›¾ä¹¦åˆ—è¡¨
        """
        award_id = self.AWARD_IDS.get(award_key)
        if not award_id:
            logger.error(f"Unknown award: {award_key}")
            return []
        
        sparql_query = self._build_sparql_query(award_id, start_year, end_year, limit)
        
        try:
            response = self._session.get(
                self._base_url,
                params={'query': sparql_query, 'format': 'json'},
                timeout=self._timeout
            )
            response.raise_for_status()
            data = response.json()
            
            return self._parse_sparql_results(data, award_key)
            
        except requests.RequestException as e:
            logger.warning(f"Failed to query Wikidata for {award_key}: {e}")
            return []
    
    def _build_sparql_query(self, award_id: str, start_year: int, 
                           end_year: int, limit: int) -> str:
        """æ„å»º SPARQL æŸ¥è¯¢è¯­å¥"""
        return f"""
        SELECT DISTINCT ?book ?bookLabel ?author ?authorLabel ?isbn13 ?isbn10 
                        ?publicationDate ?year ?publisher ?publisherLabel
        WHERE {{
          # å›¾ä¹¦è·å¾—æŒ‡å®šå¥–é¡¹
          ?book wdt:P31 wd:Q7725634 ;
                wdt:P166 wd:{award_id} ;
                wdt:P1476 ?bookLabel ;
                wdt:P50 ?author ;
                wdt:P577 ?publicationDate .
          
          # è·å–ä½œè€…åç§°
          ?author rdfs:label ?authorLabel .
          FILTER(LANG(?authorLabel) = "en")
          
          # è·å– ISBN-13ï¼ˆä¼˜å…ˆï¼‰
          OPTIONAL {{ ?book wdt:P212 ?isbn13 }}
          
          # è·å– ISBN-10
          OPTIONAL {{ ?book wdt:P957 ?isbn10 }}
          
          # è·å–å‡ºç‰ˆç¤¾
          OPTIONAL {{ 
            ?book wdt:P123 ?publisher .
            ?publisher rdfs:label ?publisherLabel .
            FILTER(LANG(?publisherLabel) = "en")
          }}
          
          # æå–å¹´ä»½
          BIND(YEAR(?publicationDate) AS ?year)
          
          # è¿‡æ»¤å¹´ä»½èŒƒå›´
          FILTER(?year >= {start_year} && ?year <= {end_year})
          
          # ç¡®ä¿æœ‰è‹±æ–‡æ ‡é¢˜
          FILTER(LANG(?bookLabel) = "en")
        }}
        ORDER BY DESC(?year)
        LIMIT {limit}
        """
    
    def _parse_sparql_results(self, data: dict, award_key: str) -> list:
        """è§£æ SPARQL æŸ¥è¯¢ç»“æœ"""
        books = []
        
        bindings = data.get('results', {}).get('bindings', [])
        
        for binding in bindings:
            book = {
                'award': award_key,
                'wikidata_id': binding.get('book', {}).get('value', '').split('/')[-1],
                'title': binding.get('bookLabel', {}).get('value', ''),
                'author_wikidata_id': binding.get('author', {}).get('value', '').split('/')[-1],
                'author': binding.get('authorLabel', {}).get('value', ''),
                'isbn13': binding.get('isbn13', {}).get('value', ''),
                'isbn10': binding.get('isbn10', {}).get('value', ''),
                'publication_date': binding.get('publicationDate', {}).get('value', ''),
                'year': int(binding.get('year', {}).get('value', 0)),
                'publisher': binding.get('publisherLabel', {}).get('value', ''),
            }
            books.append(book)
        
        return books
    
    def get_all_award_books(self, awards: list | None = None, start_year: int = 2020,
                           end_year: int = 2025) -> dict:
        """
        è·å–å¤šä¸ªå¥–é¡¹çš„è·å¥–å›¾ä¹¦
        
        Args:
            awards: å¥–é¡¹é”®ååˆ—è¡¨ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰å¥–é¡¹
            start_year: å¼€å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
            
        Returns:
            æŒ‰å¥–é¡¹åˆ†ç»„çš„å›¾ä¹¦å­—å…¸
        """
        if awards is None:
            awards = list(self.AWARD_IDS.keys())
        
        results = {}
        
        for award_key in awards:
            logger.info(f"ğŸ” æŸ¥è¯¢ {award_key} è·å¥–å›¾ä¹¦...")
            books = self.query_award_winners(award_key, start_year, end_year)
            results[award_key] = books
            logger.info(f"âœ… {award_key}: æ‰¾åˆ° {len(books)} æœ¬å›¾ä¹¦")
            
            time.sleep(0.5)
        
        return results
    
    @retry(max_attempts=2, backoff_factor=1.5)
    def query_award_info(self, award_key: str) -> dict:
        """
        æŸ¥è¯¢å¥–é¡¹çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            award_key: å¥–é¡¹é”®å (nebula, hugo, booker ç­‰)
            
        Returns:
            å¥–é¡¹ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«åç§°ã€è‹±æ–‡åç§°ã€å›½å®¶ã€è®¾ç«‹å¹´ä»½ã€æè¿°ç­‰
        """
        award_id = self.AWARD_IDS.get(award_key)
        if not award_id:
            logger.error(f"Unknown award: {award_key}")
            return {}
        
        sparql_query = self._build_award_info_query(award_id)
        
        try:
            response = self._session.get(
                self._base_url,
                params={'query': sparql_query, 'format': 'json'},
                timeout=self._timeout
            )
            response.raise_for_status()
            data = response.json()
            
            return self._parse_award_info(data, award_key)
            
        except requests.RequestException as e:
            logger.warning(f"Failed to query award info for {award_key}: {e}")
            return {}
    
    def _build_award_info_query(self, award_id: str) -> str:
        """æ„å»ºæŸ¥è¯¢å¥–é¡¹ä¿¡æ¯çš„ SPARQL è¯­å¥"""
        return f"""
        SELECT DISTINCT ?award ?awardLabel ?awardDescription 
                        ?country ?countryLabel 
                        ?inception ?categoryCount
        WHERE {{
          # å¥–é¡¹å®ä½“
          BIND(wd:{award_id} AS ?award)
          
          # è·å–æ ‡ç­¾å’Œæè¿°
          ?award rdfs:label ?awardLabel .
          FILTER(LANG(?awardLabel) = "en")
          
          ?award schema:description ?awardDescription .
          FILTER(LANG(?awardDescription) = "en")
          
          # è·å–å›½å®¶ï¼ˆå¯é€‰ï¼‰
          OPTIONAL {{ 
            ?award wdt:P17 ?country .
            ?country rdfs:label ?countryLabel .
            FILTER(LANG(?countryLabel) = "en")
          }}
          
          # è·å–è®¾ç«‹å¹´ä»½ï¼ˆå¯é€‰ï¼‰
          OPTIONAL {{ ?award wdt:P571 ?inception }}
          
          # è·å–ç±»åˆ«æ•°é‡ï¼ˆå¯é€‰ï¼‰
          OPTIONAL {{ ?award wdt:P2517 ?categoryCount }}
        }}
        LIMIT 1
        """
    
    def _parse_award_info(self, data: dict, award_key: str) -> dict:
        """è§£æå¥–é¡¹ä¿¡æ¯æŸ¥è¯¢ç»“æœ"""
        bindings = data.get('results', {}).get('bindings', [])
        
        if not bindings:
            logger.warning(f"No award info found for {award_key}")
            return {}
        
        binding = bindings[0]
        
        # æå–è®¾ç«‹å¹´ä»½
        inception = binding.get('inception', {}).get('value', '')
        established_year = None
        if inception:
            try:
                # å°è¯•ä»æ—¥æœŸå­—ç¬¦ä¸²æå–å¹´ä»½
                established_year = int(inception[:4])
            except (ValueError, IndexError):
                pass
        
        # æå–ç±»åˆ«æ•°é‡
        category_count = binding.get('categoryCount', {}).get('value', '')
        try:
            category_count = int(category_count) if category_count else None
        except ValueError:
            category_count = None
        
        return {
            'award_key': award_key,
            'wikidata_id': binding.get('award', {}).get('value', '').split('/')[-1],
            'name_en': binding.get('awardLabel', {}).get('value', ''),
            'description_en': binding.get('awardDescription', {}).get('value', ''),
            'country_en': binding.get('countryLabel', {}).get('value', ''),
            'established_year': established_year,
            'category_count': category_count,
        }
    
    def get_all_award_info(self, awards: list | None = None) -> dict:
        """
        è·å–å¤šä¸ªå¥–é¡¹çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            awards: å¥–é¡¹é”®ååˆ—è¡¨ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰å¥–é¡¹
            
        Returns:
            æŒ‰å¥–é¡¹é”®ååˆ†ç»„çš„å¥–é¡¹ä¿¡æ¯å­—å…¸
        """
        if awards is None:
            awards = list(self.AWARD_IDS.keys())
        
        results = {}
        
        for award_key in awards:
            logger.info(f"ğŸ” æŸ¥è¯¢ {award_key} å¥–é¡¹ä¿¡æ¯...")
            info = self.query_award_info(award_key)
            if info:
                results[award_key] = info
                logger.info(f"âœ… {award_key}: è·å–åˆ°å¥–é¡¹ä¿¡æ¯")
            else:
                logger.warning(f"âš ï¸ {award_key}: æœªèƒ½è·å–å¥–é¡¹ä¿¡æ¯")
            
            time.sleep(0.3)
        
        return results


class ImageCacheService:
    """å›¾ç‰‡ç¼“å­˜æœåŠ¡"""
    
    def __init__(self, cache_dir: Path, default_cover: str = '/static/default-cover.png'):
        self._cache_dir = cache_dir
        self._default_cover = default_cover
        self._cache_dir.mkdir(parents=True, exist_ok=True)
    
    def get_cached_image_url(self, original_url: str, ttl: int = 3600) -> str:
        """
        è·å–ç¼“å­˜çš„å›¾ç‰‡URL
        
        Args:
            original_url: åŸå§‹å›¾ç‰‡URL
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ç¼“å­˜åçš„å›¾ç‰‡URLæˆ–é»˜è®¤å°é¢
        """
        if not original_url:
            return self._default_cover
        
        filename = hashlib.md5(original_url.encode()).hexdigest() + '.jpg'
        cache_path = self._cache_dir / filename
        relative_path = f'/cache/images/{filename}'
        
        if cache_path.exists():
            try:
                file_age = time.time() - cache_path.stat().st_mtime
                if file_age < ttl:
                    return relative_path
            except OSError:
                pass
        
        try:
            response = requests.get(original_url, timeout=10, stream=True)
            response.raise_for_status()
            
            with open(cache_path, 'wb') as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            
            return relative_path
            
        except Exception as e:
            logger.warning(f"Failed to cache image from {original_url}: {e}")
            return self._default_cover
