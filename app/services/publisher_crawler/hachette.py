"""
Hachetteï¼ˆé˜¿æ­‡ç‰¹ï¼‰å‡ºç‰ˆç¤¾çˆ¬è™«

é˜¿æ­‡ç‰¹æ˜¯å…¨çƒç¬¬ä¸‰å¤§å‡ºç‰ˆé›†å›¢ï¼Œ
æ€»éƒ¨ä½äºæ³•å›½ï¼Œåœ¨ç¾å›½ã€è‹±å›½ç­‰åœ°æœ‰åˆ†æ”¯æœºæ„ã€‚

ç½‘ç«™ç‰¹ç‚¹ï¼š
- æŒ‰åœ°åŒºæœ‰ä¸åŒç½‘ç«™
- æä¾›æ–°ä¹¦é¢„å‘Šå’Œå‘å¸ƒä¿¡æ¯
- åˆ†ç±»æ¸…æ™°
"""
import logging
from typing import Any, Generator
from urllib.parse import urljoin

from .base_crawler import BaseCrawler, BookInfo, CrawlerConfig

logger = logging.getLogger(__name__)


class HachetteCrawler(BaseCrawler):
    """
    Hachette å‡ºç‰ˆç¤¾çˆ¬è™«

    å®˜æ–¹ç½‘ç«™ï¼šhttps://www.hachettebookgroup.com/
    æ–°ä¹¦é¡µé¢ï¼šhttps://www.hachettebookgroup.com/new-releases/
    """

    PUBLISHER_NAME = "é˜¿æ­‡ç‰¹"
    PUBLISHER_NAME_EN = "Hachette"
    PUBLISHER_WEBSITE = "https://www.hachettebookgroup.com"
    CRAWLER_CLASS_NAME = "HachetteCrawler"

    # æ–°ä¹¦é¡µé¢URL
    NEW_RELEASES_URL = "https://www.hachettebookgroup.com/new-releases/"

    # åˆ†ç±»æ˜ å°„
    CATEGORY_MAP = {
        'fiction': 'å°è¯´',
        'non-fiction': 'éè™šæ„',
        'mystery': 'æ‚¬ç–‘',
        'romance': 'è¨€æƒ…',
        'science-fiction': 'ç§‘å¹»',
        'fantasy': 'å¥‡å¹»',
        'thriller': 'æƒŠæ‚š',
        'biography': 'ä¼ è®°',
        'history': 'å†å²',
        'children': 'å„¿ç«¥è¯»ç‰©',
        'young-adult': 'é’å°‘å¹´',
        'business': 'å•†ä¸š',
        'self-help': 'è‡ªåŠ©',
    }

    def __init__(self, config: CrawlerConfig | None = None):
        super().__init__(config)
        if config is None:
            self.config.request_delay = 1.3

    def get_categories(self) -> list[dict[str, str]]:
        """è·å–æ”¯æŒçš„åˆ†ç±»åˆ—è¡¨"""
        return [
            {'id': 'fiction', 'name': 'å°è¯´'},
            {'id': 'non-fiction', 'name': 'éè™šæ„'},
            {'id': 'mystery', 'name': 'æ‚¬ç–‘'},
            {'id': 'romance', 'name': 'è¨€æƒ…'},
            {'id': 'science-fiction', 'name': 'ç§‘å¹»'},
            {'id': 'fantasy', 'name': 'å¥‡å¹»'},
            {'id': 'thriller', 'name': 'æƒŠæ‚š'},
            {'id': 'biography', 'name': 'ä¼ è®°'},
            {'id': 'history', 'name': 'å†å²'},
            {'id': 'children', 'name': 'å„¿ç«¥è¯»ç‰©'},
            {'id': 'young-adult', 'name': 'é’å°‘å¹´'},
            {'id': 'business', 'name': 'å•†ä¸š'},
            {'id': 'self-help', 'name': 'è‡ªåŠ©'},
        ]

    def get_new_books(
        self,
        category: str | None = None,
        max_books: int = 100
    ) -> Generator[BookInfo, None, None]:
        """è·å–æ–°ä¹¦åˆ—è¡¨"""
        page = 1
        count = 0

        while count < max_books and page <= self.config.max_pages:
            url = self._build_list_url(category, page)
            logger.info(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

            response = self._make_request(url)
            if not response:
                break

            soup = self._parse_html(response.text)
            books_on_page = self._parse_book_list(soup)

            if not books_on_page:
                logger.info(f"ğŸ“– ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šä¹¦ç±")
                break

            for book_data in books_on_page:
                if count >= max_books:
                    break

                book_info = self.get_book_details(book_data['url'])
                if book_info:
                    yield book_info
                    count += 1

            page += 1

    def _build_list_url(self, category: str | None, page: int) -> str:
        """æ„å»ºåˆ—è¡¨é¡µURL"""
        params = []

        if category:
            params.append(f"category={category}")

        if page > 1:
            params.append(f"page={page}")

        if params:
            return f"{self.NEW_RELEASES_URL}?{'&'.join(params)}"
        return self.NEW_RELEASES_URL

    def _parse_book_list(self, soup) -> list[dict[str, str]]:
        """è§£æä¹¦ç±åˆ—è¡¨é¡µ"""
        books = []

        # Hachette ä¹¦ç±åˆ—è¡¨é€‰æ‹©å™¨
        book_items = soup.select('.book-item, .product-card, [data-book-id], .book-card')

        if not book_items:
            book_items = soup.select('article.book, li.book, div.book')

        for item in book_items:
            try:
                book_data = {}

                # æå–è¯¦æƒ…é“¾æ¥
                link = item.select_one('a[href*="/book/"], a[href*="/books/"]')
                if link:
                    href = link.get('href', '')
                    book_data['url'] = urljoin(self.PUBLISHER_WEBSITE, href)

                # æå–ä¹¦å
                title_elem = item.select_one('.title, .book-title, h2, h3')
                if title_elem:
                    book_data['title'] = self._clean_text(title_elem.get_text())

                # æå–ä½œè€…
                author_elem = item.select_one('.author, .book-author, .contributor')
                if author_elem:
                    book_data['author'] = self._clean_text(author_elem.get_text())

                if book_data.get('url'):
                    books.append(book_data)

            except Exception as e:
                logger.warning(f"âš ï¸ è§£æä¹¦ç±é¡¹å¤±è´¥: {e}")
                continue

        logger.info(f"ğŸ“– åœ¨å½“å‰é¡µé¢æ‰¾åˆ° {len(books)} æœ¬ä¹¦ç±")
        return books

    def get_book_details(self, book_url: str) -> BookInfo | None:
        """è·å–ä¹¦ç±è¯¦æƒ…"""
        response = self._make_request(book_url)
        if not response:
            return None

        soup = self._parse_html(response.text)

        try:
            book_info = BookInfo(
                title=self._extract_title(soup),
                author=self._extract_author(soup),
                isbn13=None,
                isbn10=None,
                description=self._extract_description(soup),
                cover_url=self._extract_cover_url(soup),
                category=self._extract_category(soup),
                publication_date=self._extract_publication_date(soup),
                price=self._extract_price(soup),
                page_count=self._extract_page_count(soup),
                language='English',
                buy_links=self._extract_buy_links(soup),
                source_url=book_url,
            )

            isbn_text = self._extract_isbn_text(soup)
            if isbn_text:
                book_info.isbn13, book_info.isbn10 = self._extract_isbn(isbn_text)

            book_info.description = self._truncate_description(book_info.description)

            return book_info

        except Exception as e:
            logger.error(f"âŒ è§£æä¹¦ç±è¯¦æƒ…å¤±è´¥ {book_url}: {e}")
            return None

    def _extract_title(self, soup) -> str:
        """æå–ä¹¦å"""
        selectors = ['.book-title', '.product-title', 'h1.title', 'h1.book-name', 'h1']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return self._clean_text(elem.get_text())
        return "Unknown Title"

    def _extract_author(self, soup) -> str:
        """æå–ä½œè€…"""
        selectors = ['.author-name', '.book-author', '.contributor-name', '.author a']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return self._clean_text(elem.get_text())
        return "Unknown Author"

    def _extract_description(self, soup) -> str | None:
        """æå–ç®€ä»‹"""
        selectors = ['.book-description', '.product-description', '.synopsis', '.summary', '.description']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return self._clean_text(elem.get_text())
        return None

    def _extract_cover_url(self, soup) -> str | None:
        """æå–å°é¢URL"""
        img_selectors = ['.book-cover img', '.product-image img', '.cover-image img', 'img.book-image']
        for selector in img_selectors:
            img = soup.select_one(selector)
            if img:
                src = img.get('src') or img.get('data-src')
                if src:
                    return urljoin(self.PUBLISHER_WEBSITE, src)
        return None

    def _extract_category(self, soup) -> str | None:
        """æå–åˆ†ç±»"""
        selectors = ['.category', '.genre', '.book-category', '.imprint']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                category_en = self._clean_text(elem.get_text()).lower()
                return self.CATEGORY_MAP.get(category_en, category_en)
        return None

    def _extract_publication_date(self, soup) -> Any:
        """æå–å‡ºç‰ˆæ—¥æœŸ"""
        selectors = ['.publication-date', '.release-date', '.publish-date', '.on-sale-date']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                date_text = self._clean_text(elem.get_text())
                return self._parse_date(date_text)

        page_text = soup.get_text()
        import re
        patterns = [
            r'(?:On Sale|Publication Date|Release Date|Pub Date)[:\s]+([A-Za-z]+\s+\d{1,2},?\s+\d{4})',
            r'(?:On Sale|Publication Date|Release Date|Pub Date)[:\s]+(\d{1,2}[/-]\d{1,2}[/-]\d{4})',
        ]
        for pattern in patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                return self._parse_date(match.group(1))

        return None

    def _extract_price(self, soup) -> str | None:
        """æå–ä»·æ ¼"""
        selectors = ['.price', '.book-price', '.product-price']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return self._parse_price(elem.get_text())
        return None

    def _extract_page_count(self, soup) -> int | None:
        """æå–é¡µæ•°"""
        selectors = ['.page-count', '.pages', '.book-pages']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text()
                import re
                match = re.search(r'(\d+)', text)
                if match:
                    return int(match.group(1))
        return None

    def _extract_isbn_text(self, soup) -> str | None:
        """æå–åŒ…å«ISBNçš„æ–‡æœ¬"""
        selectors = ['.isbn', '.book-isbn', '[data-isbn]', '.product-details']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text()

        page_text = soup.get_text()
        import re
        match = re.search(r'ISBN[-:\s]*(97[89]\d{10}|\d{9}[\dXx])', page_text, re.IGNORECASE)
        if match:
            return match.group(0)

        return None

    def _extract_buy_links(self, soup) -> list[dict[str, str]]:
        """æå–è´­ä¹°é“¾æ¥"""
        links = []

        retailers = {
            'Amazon': 'amazon.com',
            'Barnes & Noble': 'bn.com',
            'Books-A-Million': 'booksamillion.com',
            'Bookshop': 'bookshop.org',
            'IndieBound': 'indiebound.org',
            'Target': 'target.com',
            'Walmart': 'walmart.com',
        }

        buy_section = soup.select_one('.buy-buttons, .purchase-options, .buy-links, .retailers')
        if buy_section:
            for link in buy_section.find_all('a', href=True):
                href = link.get('href', '')
                text = self._clean_text(link.get_text())

                for name, domain in retailers.items():
                    if domain in href:
                        links.append({'name': name, 'url': href})
                        break

        return links
