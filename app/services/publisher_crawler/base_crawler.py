"""
å‡ºç‰ˆç¤¾çˆ¬è™«åŸºç±»

æä¾›çˆ¬è™«çš„é€šç”¨åŠŸèƒ½ï¼š
- HTTPè¯·æ±‚å¤„ç†ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
- robots.txt éµå®ˆ
- åˆ†é¡µå¤„ç†
- é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""
import time
import logging
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import date, datetime, timezone
from typing import Any, Generator
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger(__name__)


@dataclass
class BookInfo:
    """
    ä¹¦ç±ä¿¡æ¯æ•°æ®ç±»

    ç”¨äºå­˜å‚¨ä»å‡ºç‰ˆç¤¾ç½‘ç«™çˆ¬å–çš„åŸå§‹ä¹¦ç±æ•°æ®ã€‚
    """
    title: str
    author: str
    isbn13: str | None = None
    isbn10: str | None = None
    description: str | None = None
    cover_url: str | None = None
    category: str | None = None
    publication_date: date | None = None
    price: str | None = None
    page_count: int | None = None
    language: str | None = None
    buy_links: list[dict[str, str]] = field(default_factory=list)
    source_url: str | None = None

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'title': self.title,
            'author': self.author,
            'isbn13': self.isbn13,
            'isbn10': self.isbn10,
            'description': self.description,
            'cover_url': self.cover_url,
            'category': self.category,
            'publication_date': self.publication_date.isoformat() if self.publication_date else None,
            'price': self.price,
            'page_count': self.page_count,
            'language': self.language,
            'buy_links': self.buy_links,
            'source_url': self.source_url,
        }


@dataclass
class CrawlerConfig:
    """
    çˆ¬è™«é…ç½®æ•°æ®ç±»
    """
    # è¯·æ±‚é…ç½®
    timeout: int = 15
    max_retries: int = 3
    retry_delay: float = 2.0
    request_delay: float = 1.0  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

    # åˆ†é¡µé…ç½®
    max_pages: int = 10  # æœ€å¤§çˆ¬å–é¡µæ•°
    page_size: int = 20  # æ¯é¡µæ•°é‡

    # å†…å®¹é…ç½®
    max_description_length: int = 2000  # ç®€ä»‹æœ€å¤§é•¿åº¦

    # User-Agent
    user_agent: str = 'BookRank/3.0 (https://github.com/gongyijie85/bookrank)'

    # API Keyï¼ˆç”¨äºéœ€è¦è®¤è¯çš„APIï¼‰
    api_key: str | None = None

    # æ˜¯å¦éµå®ˆ robots.txt
    respect_robots_txt: bool = True


class BaseCrawler(ABC):
    """
    å‡ºç‰ˆç¤¾çˆ¬è™«æŠ½è±¡åŸºç±»

    æ‰€æœ‰å‡ºç‰ˆç¤¾çˆ¬è™«éƒ½éœ€è¦ç»§æ‰¿æ­¤ç±»å¹¶å®ç°æŠ½è±¡æ–¹æ³•ã€‚
    """

    # å‡ºç‰ˆç¤¾ä¿¡æ¯ï¼ˆå­ç±»éœ€è¦è¦†ç›–ï¼‰
    PUBLISHER_NAME: str = ""
    PUBLISHER_NAME_EN: str = ""
    PUBLISHER_WEBSITE: str = ""
    CRAWLER_CLASS_NAME: str = ""

    def __init__(self, config: CrawlerConfig | None = None):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            config: çˆ¬è™«é…ç½®ï¼Œä¸º None æ—¶ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or CrawlerConfig()
        self._session = self._create_session()
        self._robots_parser: RobotFileParser | None = None
        self._is_allowed_by_robots = True

        # åˆå§‹åŒ– robots.txt è§£æå™¨
        if self.config.respect_robots_txt and self.PUBLISHER_WEBSITE:
            self._init_robots_parser()

    def _create_session(self) -> requests.Session:
        """
        åˆ›å»ºé…ç½®äº†é‡è¯•æœºåˆ¶çš„ requests Session

        Returns:
            é…ç½®å¥½çš„ Session å¯¹è±¡
        """
        session = requests.Session()

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=self.config.max_retries,
            backoff_factor=self.config.retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )

        # é…ç½®è¿æ¥æ± 
        adapter = HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=retry_strategy
        )

        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
        session.headers.update({
            'User-Agent': self.config.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })

        return session

    def _init_robots_parser(self) -> None:
        """åˆå§‹åŒ– robots.txt è§£æå™¨"""
        try:
            robots_url = urljoin(self.PUBLISHER_WEBSITE, '/robots.txt')
            self._robots_parser = RobotFileParser()
            self._robots_parser.set_url(robots_url)
            self._robots_parser.read()
            logger.info(f"âœ… å·²åŠ è½½ robots.txt: {robots_url}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•åŠ è½½ robots.txt: {e}")
            self._robots_parser = None

    def _is_url_allowed(self, url: str) -> bool:
        """
        æ£€æŸ¥ URL æ˜¯å¦è¢« robots.txt å…è®¸

        Args:
            url: è¦æ£€æŸ¥çš„ URL

        Returns:
            æ˜¯å¦å…è®¸è®¿é—®
        """
        if not self._robots_parser:
            return True

        try:
            return self._robots_parser.can_fetch(self.config.user_agent, url)
        except Exception:
            return True

    def _make_request(
        self,
        url: str,
        method: str = 'GET',
        **kwargs
    ) -> requests.Response | None:
        """
        å‘é€ HTTP è¯·æ±‚ï¼ˆå¸¦é‡è¯•å’Œå»¶è¿Ÿï¼‰

        Args:
            url: è¯·æ±‚ URL
            method: è¯·æ±‚æ–¹æ³•
            **kwargs: ä¼ é€’ç»™ requests çš„å…¶ä»–å‚æ•°

        Returns:
            Response å¯¹è±¡æˆ– None
        """
        # æ£€æŸ¥ robots.txt
        if not self._is_url_allowed(url):
            logger.warning(f"âš ï¸ robots.txt ç¦æ­¢è®¿é—®: {url}")
            return None

        # è®¾ç½®è¶…æ—¶
        kwargs.setdefault('timeout', self.config.timeout)

        try:
            # è¯·æ±‚å‰å»¶è¿Ÿ
            time.sleep(self.config.request_delay)

            response = self._session.request(method, url, **kwargs)
            response.raise_for_status()

            logger.info(f"âœ… è¯·æ±‚æˆåŠŸ: {url} (çŠ¶æ€ç : {response.status_code})")
            return response

        except requests.Timeout:
            logger.error(f"âŒ è¯·æ±‚è¶…æ—¶: {url}")
            return None
        except requests.HTTPError as e:
            logger.error(f"âŒ HTTPé”™è¯¯: {url} - {e}")
            return None
        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {url} - {e}")
            return None

    def _parse_html(self, html: str, parser: str = 'html.parser') -> BeautifulSoup:
        """
        è§£æ HTML å†…å®¹

        Args:
            html: HTML å­—ç¬¦ä¸²
            parser: BeautifulSoup è§£æå™¨

        Returns:
            BeautifulSoup å¯¹è±¡
        """
        return BeautifulSoup(html, parser)

    def _clean_text(self, text: str | None) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºç™½å’Œæ¢è¡Œï¼‰

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""

        # å»é™¤å¤šä½™ç©ºç™½å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _extract_isbn(self, text: str) -> tuple[str | None, str | None]:
        """
        ä»æ–‡æœ¬ä¸­æå– ISBN-13 å’Œ ISBN-10

        Args:
            text: åŒ…å« ISBN çš„æ–‡æœ¬

        Returns:
            (isbn13, isbn10) å…ƒç»„
        """
        isbn13 = None
        isbn10 = None

        # æå– ISBN-13ï¼ˆ13ä½æ•°å­—ï¼Œå¯èƒ½ä»¥978æˆ–979å¼€å¤´ï¼‰
        isbn13_match = re.search(r'(?:ISBN[-:\s]*)?(97[89]\d{10})', text, re.IGNORECASE)
        if isbn13_match:
            isbn13 = isbn13_match.group(1)

        # æå– ISBN-10ï¼ˆ10ä½ï¼Œæœ€åä¸€ä½å¯èƒ½æ˜¯Xï¼‰
        isbn10_match = re.search(r'(?:ISBN[-:\s]*)?(\d{9}[\dXx])(?!\d)', text, re.IGNORECASE)
        if isbn10_match and not isbn13:
            isbn10 = isbn10_match.group(1).upper()

        return isbn13, isbn10

    def _parse_date(self, date_str: str | None) -> date | None:
        """
        è§£ææ—¥æœŸå­—ç¬¦ä¸²

        æ”¯æŒå¤šç§å¸¸è§æ ¼å¼ï¼š
        - YYYY-MM-DD
        - YYYY/MM/DD
        - Month DD, YYYY
        - DD Month YYYY

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²

        Returns:
            date å¯¹è±¡æˆ– None
        """
        if not date_str:
            return None

        date_str = self._clean_text(date_str)

        # å¸¸è§æ—¥æœŸæ ¼å¼
        formats = [
            '%Y-%m-%d',
            '%Y/%m/%d',
            '%m/%d/%Y',
            '%d/%m/%Y',
            '%B %d, %Y',      # January 15, 2024
            '%b %d, %Y',      # Jan 15, 2024
            '%d %B %Y',       # 15 January 2024
            '%d %b %Y',       # 15 Jan 2024
            '%Y',             # ä»…å¹´ä»½
        ]

        for fmt in formats:
            try:
                parsed = datetime.strptime(date_str, fmt)
                return parsed.date()
            except ValueError:
                continue

        logger.warning(f"âš ï¸ æ— æ³•è§£ææ—¥æœŸ: {date_str}")
        return None

    def _parse_price(self, price_str: str | None) -> str | None:
        """
        è§£æä»·æ ¼å­—ç¬¦ä¸²

        Args:
            price_str: ä»·æ ¼å­—ç¬¦ä¸²

        Returns:
            æ ¼å¼åŒ–åçš„ä»·æ ¼
        """
        if not price_str:
            return None

        price_str = self._clean_text(price_str)

        # æå–æ•°å­—å’Œè´§å¸ç¬¦å·
        match = re.search(r'([\$â‚¬Â£Â¥]?\s*[\d,]+\.?\d*)', price_str)
        if match:
            return match.group(1).strip()

        return price_str

    def _truncate_description(self, description: str | None) -> str | None:
        """
        æˆªæ–­ç®€ä»‹åˆ°æœ€å¤§é•¿åº¦

        Args:
            description: åŸå§‹ç®€ä»‹

        Returns:
            æˆªæ–­åçš„ç®€ä»‹
        """
        if not description:
            return None

        if len(description) <= self.config.max_description_length:
            return description

        return description[:self.config.max_description_length - 3] + '...'

    @abstractmethod
    def get_new_books(
        self,
        category: str | None = None,
        max_books: int = 100
    ) -> Generator[BookInfo, None, None]:
        """
        è·å–æ–°ä¹¦åˆ—è¡¨ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            category: åˆ†ç±»ç­›é€‰ï¼ˆå¯é€‰ï¼‰
            max_books: æœ€å¤§è·å–æ•°é‡

        Yields:
            BookInfo å¯¹è±¡
        """
        pass

    @abstractmethod
    def get_book_details(self, book_url: str) -> BookInfo | None:
        """
        è·å–ä¹¦ç±è¯¦æƒ…ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            book_url: ä¹¦ç±è¯¦æƒ…é¡µ URL

        Returns:
            BookInfo å¯¹è±¡æˆ– None
        """
        pass

    @abstractmethod
    def get_categories(self) -> list[dict[str, str]]:
        """
        è·å–æ”¯æŒçš„åˆ†ç±»åˆ—è¡¨ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰

        Returns:
            åˆ†ç±»åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« id å’Œ name
        """
        pass

    def crawl(
        self,
        category: str | None = None,
        max_books: int = 100
    ) -> list[BookInfo]:
        """
        æ‰§è¡Œçˆ¬å–ä»»åŠ¡

        Args:
            category: åˆ†ç±»ç­›é€‰
            max_books: æœ€å¤§è·å–æ•°é‡

        Returns:
            ä¹¦ç±ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"ğŸ” å¼€å§‹çˆ¬å– {self.PUBLISHER_NAME} æ–°ä¹¦...")

        books = []
        count = 0

        try:
            for book_info in self.get_new_books(category=category, max_books=max_books):
                books.append(book_info)
                count += 1

                if count >= max_books:
                    break

                # è¿›åº¦æ—¥å¿—
                if count % 10 == 0:
                    logger.info(f"ğŸ“– å·²çˆ¬å– {count} æœ¬ä¹¦ç±...")

        except Exception as e:
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

        logger.info(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(books)} æœ¬ä¹¦ç±")
        return books

    def close(self) -> None:
        """å…³é—­çˆ¬è™«ï¼Œé‡Šæ”¾èµ„æº"""
        if self._session:
            self._session.close()
            logger.info(f"ğŸ”’ å·²å…³é—­ {self.PUBLISHER_NAME} çˆ¬è™«")

    def __enter__(self) -> 'BaseCrawler':
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()
